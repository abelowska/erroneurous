{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a73099-38c1-4bf0-afd3-5481df7c3d19",
   "metadata": {},
   "source": [
    "# Prediction of anxiety-related phenomena fron error-related brain signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff9f56-5483-44c7-b9cc-486a31838279",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e84d26-b32c-41ae-b3f4-81e0ee55156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import ast\n",
    "import os.path as op\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "import pywt\n",
    "import mne\n",
    "import scipy\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import cesium.featurize\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import Dropdown, FloatRangeSlider, IntSlider, FloatSlider, interact\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from mne.decoding import SPoC\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from transformers import *\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c69899-6a32-4418-9079-8bd66aed497f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Load data\n",
    "\n",
    "Loading EEG data and data from rumination questionnaire. By default create_df_data loads all info from given file but one can specify it by passing a list of desired labels from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc483a7-410f-4b65-9955-9df42c783bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths TODO\n",
    "dir_path = os.path.dirname(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb04d5",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a25389-d030-415b-982d-b68df981b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = -0.1, 0.6  # Start and end of the segments\n",
    "\n",
    "signal_frequency = 256\n",
    "ERROR = 0\n",
    "CORRECT = 1\n",
    "random_state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c412f36-66a2-4fd8-bfa7-caa98c4a315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_data(\n",
    "    test_participants=False,\n",
    "    test_epochs=False,\n",
    "    info_filename=None,\n",
    "    info=\"all\",\n",
    "    personal=True,\n",
    "):\n",
    "    \"\"\"Loads data for all participants and create DataFrame with optional additional info from given .csv file.\n",
    "\n",
    "    On default, loads a train set: chooses only 80% of participants\n",
    "    and for each of them chooses 80% of epochs.\n",
    "    It will choose them deterministically.\n",
    "\n",
    "    Participants with less than 10 epochs per condition are rejected.\n",
    "\n",
    "    If test_participants is set to True, it will load remaining 20% of participants.\n",
    "    If test_epochs is set to True, it will load remaining 20% of epochs.\n",
    "    Test epochs are chronologically after train epochs,\n",
    "    because it reflects real usage (first callibration and then classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_participants: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load participants data for testing.\n",
    "    test_epochs: bool\n",
    "        whether load data for training or final testing.\n",
    "        If true load epochs of each participants data for testing.\n",
    "    info_filename: String | None\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "    personal: bool\n",
    "        whether a model will be both trained and tested on epochs from one person\n",
    "        if false, person's epochs aren't split into test and train\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go_nogo_data_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    print(os.path.abspath(\"\"))\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"\"))\n",
    "    print(dir_path)\n",
    "    header_files_glob = os.path.join(dir_path, \"data/responses_100_600/*.vhdr\")\n",
    "    header_files = glob.glob(header_files_glob)\n",
    "\n",
    "    header_files = sorted(header_files)\n",
    "    go_nogo_data_df = pd.DataFrame()\n",
    "\n",
    "    # cut 20% of data for testing\n",
    "    h_train, h_test = train_test_split(header_files, test_size=0.3, random_state=0)\n",
    "    \n",
    "    print(f\"train size: {len(h_train)} ; test size: {len(h_test)}\")\n",
    "\n",
    "    if test_participants:\n",
    "        header_files = h_test\n",
    "    else:\n",
    "        header_files = h_train\n",
    "\n",
    "    for file in header_files:\n",
    "        #  load eeg data for given participant\n",
    "        participant_epochs = load_epochs_from_file(file)\n",
    "\n",
    "        # and compute participant's id from file_name\n",
    "        participant_id = re.match(r\".*_(\\w+).*\", file).group(1)\n",
    "\n",
    "        error = participant_epochs[\"error_response\"]._data\n",
    "        correct = participant_epochs[\"correct_response\"]._data\n",
    "\n",
    "        # exclude those participants who have too few samples\n",
    "        if len(error) < 5 or len(correct) < 5:\n",
    "            # not enough data for this participant\n",
    "            continue\n",
    "\n",
    "        # construct dataframe for participant with: id|epoch_data|response_type|additional info...\n",
    "        participant_df = create_df_from_epochs(\n",
    "            participant_id, participant_epochs, info_filename, info\n",
    "        )\n",
    "        print(participant_id)\n",
    "        go_nogo_data_df = go_nogo_data_df.append(participant_df, ignore_index=True)\n",
    "\n",
    "    return go_nogo_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0991419b-7acd-42bc-b048-e5e9fa3cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_epochs(id, participant_epochs, info_filename, info):\n",
    "    \"\"\"Create df for each participant. DF structure is like: {id: String ; epoch: epoch_data ; marker: 1.0|0.0}\n",
    "    1.0 means correct and 0.0 means error response.\n",
    "    Default info extracted form .csv file is 'Rumination Full Scale' and participants' ids.\n",
    "    With this info df structure is like:\n",
    "    {id: String ; epoch: epoch_data ; marker: 1.0|0.0 ; File: id ; 'Rumination Full Scale': int}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    id: String\n",
    "        participant's id extracted from filename\n",
    "    correct: array\n",
    "        correct responses' data\n",
    "    error: array\n",
    "        error responses' data\n",
    "    info_filename: String\n",
    "        path to .csv file with additional data.\n",
    "    info: array\n",
    "        listed parameters from the info file to be loaded.\n",
    "        if 'all', load all parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    participant_df : pandas.DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    participant_df = pd.DataFrame()\n",
    "    info_df = pd.DataFrame()\n",
    "\n",
    "    # get additional info from file\n",
    "    if info_filename is not None:\n",
    "        if info == \"all\":\n",
    "            rumination_df = pd.read_csv(info_filename)\n",
    "        else:\n",
    "            rumination_df = pd.read_csv(info_filename, usecols=[\"Demo_kod\"] + info)\n",
    "        info_df = (\n",
    "            rumination_df.loc[rumination_df[\"Demo_kod\"] == id]\n",
    "            .reset_index()\n",
    "            .drop(\"index\", axis=1)\n",
    "        )\n",
    "        \n",
    "    epoch_df = pd.DataFrame({\"id\": [id], \"epoch\": [participant_epochs], \"marker\": [ALL]}).join(\n",
    "            info_df\n",
    "        )\n",
    "    participant_df = participant_df.append(epoch_df, ignore_index=True)\n",
    "\n",
    "    return participant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa88517e-884a-4b5b-9652-1bf2e7787aef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_epochs_from_file(file, reject_bad_segments=\"auto\", mask=None):\n",
    "    \"\"\"Load epochs from a header file.\n",
    "\n",
    "    Args:\n",
    "        file: path to a header file (.vhdr)\n",
    "        reject_bad_segments: 'auto' means that bad segments are rejected automatically.\n",
    "\n",
    "    Returns:\n",
    "        mne Epochs\n",
    "\n",
    "    \"\"\"\n",
    "    # Import the BrainVision data into an MNE Raw object\n",
    "    raw = mne.io.read_raw_brainvision(file)\n",
    "\n",
    "    # Construct annotation filename\n",
    "    annot_file = file[:-4] + \"vmrk\"\n",
    "\n",
    "    # Read in the event information as MNE annotations\n",
    "    annotations = mne.read_annotations(annot_file)\n",
    "\n",
    "    # Add the annotations to our raw object so we can use them with the data\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Map with response markers only\n",
    "    event_dict = {\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FB\": 10004,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_1*R*FG\": 10005,\n",
    "        \"Stimulus/RE*ex*1_n*1_c_2*R\": 10006,\n",
    "        \"Stimulus/RE*ex*1_n*2_c_1*R\": 10007,\n",
    "        \"Stimulus/RE*ex*2_n*1_c_1*R\": 10008,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FB\": 10009,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_1*R*FG\": 10010,\n",
    "        \"Stimulus/RE*ex*2_n*2_c_2*R\": 10011,\n",
    "    }\n",
    "\n",
    "    # Map for merged correct/error response markers\n",
    "    merged_event_dict = {\"correct_response\": 0, \"error_response\": 1}\n",
    "\n",
    "    # Reconstruct the original events from Raw object\n",
    "    events, event_ids = mne.events_from_annotations(raw, event_id=event_dict)\n",
    "\n",
    "    # Merge correct/error response events\n",
    "    merged_events = mne.merge_events(\n",
    "        events,\n",
    "        [10004, 10005, 10009, 10010],\n",
    "        merged_event_dict[\"correct_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "    merged_events = mne.merge_events(\n",
    "        merged_events,\n",
    "        [10006, 10007, 10008, 10011],\n",
    "        merged_event_dict[\"error_response\"],\n",
    "        replace_events=True,\n",
    "    )\n",
    "\n",
    "    epochs = []\n",
    "    bads = []\n",
    "    this_reject_by_annotation = True\n",
    "    \n",
    "    # maximum acceptable peak-to-peak amplitudes\n",
    "    reject_criteria = dict(eeg=150e-6)       # 200 µV\n",
    "    \n",
    "    # minimum acceptable peak-to-peak amplitudes\n",
    "    flat_criteria = dict(eeg=1e-6)           # 1 µV\n",
    "    \n",
    "    picks_eeg = mne.pick_types(raw.info, meg=False, eeg=True, eog=False,\n",
    "                           stim=False, exclude='bads', selection=red_box7_prim)\n",
    "\n",
    "    # Read epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw,\n",
    "        events=merged_events,\n",
    "        event_id=merged_event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        reject_by_annotation=this_reject_by_annotation,\n",
    "        preload=True,\n",
    "        # verbose='CRITICAL',\n",
    "    )\n",
    "    \n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5e07d-9618-47de-a706-687a529fb2fe",
   "metadata": {},
   "source": [
    "#### Read the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db8c673-5477-4126-b09d-870a8fcd9477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled file found. Loading pickled data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_name = \"go_nogo_100_600_df_3-5_all_scales\"\n",
    "pickled_data_filename = \"../data/responses_100_600_pickled/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/all_scales.csv\"\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "    pass\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_df = create_df_data(\n",
    "        test_participants=False, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_df.to_pickle(\"../data/\" + epochs_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055815fa-0d47-4bdd-9caf-51d60ced5d91",
   "metadata": {},
   "source": [
    "#### Read the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c990ac1-14fc-4d0c-bf5d-d96f076e1e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled file found. Loading pickled data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_name = \"go_nogo_100_600_test_df_3-5_all_scales\"\n",
    "pickled_data_filename = \"../data/responses_100_600_pickled/\" + df_name + \".pkl\"\n",
    "info_filename = \"../data/scales/all_scales.csv\"\n",
    "\n",
    "\n",
    "# Check if data is already loaded\n",
    "if os.path.isfile(pickled_data_filename):\n",
    "    print(\"Pickled file found. Loading pickled data...\")\n",
    "    epochs_test_df = pd.read_pickle(pickled_data_filename)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Pickled file not found. Loading data...\")\n",
    "    epochs_test_df = create_df_data(\n",
    "        test_participants=True, info=\"all\", personal=False, info_filename=info_filename\n",
    "    )\n",
    "    epochs_test_df.name = df_name\n",
    "    # save loaded data into a pickle file\n",
    "    epochs_test_df.to_pickle(\"../data/responses_100_600_pickled/\" + epochs_test_df.name + \".pkl\")\n",
    "    print(\"Done. Pickle file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea2478-01a2-499e-b6bb-b20e580c0676",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set-up for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf38e4",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a593d8-7928-410c-88b9-0b4b25f1c493",
   "metadata": {},
   "source": [
    "#### Create X train and y train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7afda6e-cc8b-492f-bb80-ac3a0b0cde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of the analysed condition: erroneous responses or correct responses\n",
    "dataset = ERROR\n",
    "\n",
    "dataset_name = \"correct_response\" if dataset == CORRECT else \"error_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "484eb207-0fc2-495f-aa9f-380ecc815b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training set: (96, 163)\n",
      "Shape of the testing set: (34, 163)\n"
     ]
    }
   ],
   "source": [
    "X_train_df = epochs_df\n",
    "X_test_df = epochs_test_df\n",
    "\n",
    "print(f\"Shape of the training set: {X_train_df.shape}\")\n",
    "print(f\"Shape of the testing set: {X_test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2738f8",
   "metadata": {},
   "source": [
    "#### Fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eabcff2-1d75-4aac-bda1-9565705b9707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                               0\n",
       "epoch                                                                                                            0\n",
       "marker                                                                                                           0\n",
       "Demo_kod                                                                                                         1\n",
       "Pula badań                                                                                                       0\n",
       "                                                                                                                ..\n",
       "35-IN-2.\\tJaką część czasu spędzonego on-line poświęcasz na pracę lub naukę?                                     0\n",
       "35-IN-3.\\tJaką część czasu spędzonego on-line poświęcasz na media społecznościowe?                               0\n",
       "35-IN-4.\\tJaką część czasu spędzonego on-line poświęcasz na granie w gry komputerowe?                            0\n",
       "35-IN-5.\\tJaką część czasu spędzonego on-line poświęcasz na oglądanie filmów, czytanie prasy i inne rozrywki?    0\n",
       "36-Raven                                                                                                         1\n",
       "Length: 163, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = X_train_df.fillna(X_train_df.mean())\n",
    "\n",
    "X_train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd7a82",
   "metadata": {},
   "source": [
    "#### Select questionnaires for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa6f1ac9-2bd1-46c6-b404-a394ac24603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape 1-D: scores\n",
    "rumination = \"16-Rumination Full Scale\"\n",
    "dass_anxiety = \"05-DASS-21 Anxiety scale\"\n",
    "stai_t = \"04-STAI Trait SUM\" \n",
    "bis = \"07-BIS\"\n",
    "bas_dzialanie = \"07-BAS Dzialanie\"\n",
    "bas_przyjemnosc = \"07-BAS Poszukiwanie przyjemnosci\"\n",
    "bas_nagroda = \"07-BAS Wrazliwosc na nagrode\"\n",
    "washing = \"14-Obsessive-Compulsive WASHING\"\n",
    "obsessing = \"14-Obsessive-Compulsive OBSESSING\"\n",
    "hoarding = \"14-Obsessive-Compulsive HOARDING\"\n",
    "ordering = \"14-Obsessive-Compulsive ORDERING\"\n",
    "checking = \"14-Obsessive-Compulsive CHECKING\"\n",
    "neutralizing = \"14-Obsessive-Compulsive NEUTRALIZING\"\n",
    "oci_r_full = \"14-Obsessive-Compulsive FULL\"\n",
    "threat = \"15-Obsessional Beliefs - Overestimation of threat\"\n",
    "perfectionism_IU = \"15-Obsessional Beliefs - Perfectionism/ Intolerance of uncertainty\"\n",
    "thought_suppression = \"18-Thought Suppression Inventory\"\n",
    "nonforgivness = \"22-Nonforgiveness - Full Scale\"\n",
    "indecisivness = \"27-Indecisiveness Scale_Frost\"\n",
    "IU_prospecitve = \"28-Intolerance of Uncertainty - Prospective Anxiety\"\n",
    "IU_inhibitory = \"28-Intolerance of Uncertainty - Inhibitory Anxiety\"\n",
    "self_esteem = \"06-Self-Esteem Scale_SES Rosenberga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fb80bb5-46bd-4aba-b5d9-8402f97ea6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [\n",
    "    rumination,\n",
    "    dass_anxiety,\n",
    "    stai_t,\n",
    "    bis,\n",
    "    washing,\n",
    "    obsessing,\n",
    "    hoarding,\n",
    "    ordering,\n",
    "    checking,\n",
    "    neutralizing,\n",
    "    oci_r_full,\n",
    "    threat,\n",
    "    thought_suppression,\n",
    "    IU_prospecitve,\n",
    "    IU_inhibitory,\n",
    "    self_esteem,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366c3c6-dd06-4837-9bd8-a286d9c530f6",
   "metadata": {},
   "source": [
    "### Prepare experiments \n",
    "\n",
    "Parameters of experiments:\n",
    "- regressors\n",
    "- hyperparameters\n",
    "- preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c75fe861-b8ef-4c53-a68c-5241ba5348ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating model with grid search\n",
    "\n",
    "def rate_regressor(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    regressor, \n",
    "    regressor_params, \n",
    "    base_steps, \n",
    "    cv=3\n",
    "):\n",
    "    \n",
    "    # define cross-validation method\n",
    "    cv_kf = KFold(n_splits=3)\n",
    "\n",
    "    pipeline = Pipeline(base_steps + [regressor])\n",
    "    param_grid = regressor_params\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=cv_kf,\n",
    "        scoring={\"r2\", \"neg_mean_absolute_error\", \"neg_mean_squared_error\"},\n",
    "        refit=\"r2\",\n",
    "        return_train_score=True,\n",
    "        n_jobs=10,\n",
    "        verbose=10,\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38bfa7df-bda2-4249-97c5-999e73d1abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating p-value with permutation test from sci-kit learn\n",
    "\n",
    "def calculate_p_permutations(estimator, X, y, cv=3, n_permutations=1000, n_jobs=10):\n",
    "\n",
    "    score_, perm_scores_, pvalue_ = permutation_test_score(\n",
    "        estimator, X, y, cv=cv, n_permutations=n_permutations, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    # summarize\n",
    "    print(f\"     The permutation P-value is = {pvalue_:.4f}\")\n",
    "    print(f\"     The permutation score is = {score_:.4f}\\n\")\n",
    "\n",
    "    return score_, pvalue_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e3fe26a-00a6-41cf-94c1-2fb101214e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-implemented permutation test for testing significance of tests on hold-out dataset\n",
    "\n",
    "def permutation_test_score_external(\n",
    "    estimator,\n",
    "    X,\n",
    "    y,\n",
    "    *,\n",
    "    groups=None,\n",
    "    cv=None,\n",
    "    n_permutations=100,\n",
    "    n_jobs=None,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    scoring=None,\n",
    "    fit_params=None,\n",
    "):\n",
    "    score = estimator.score(X, y)\n",
    "    print(f\"score: {score} \")\n",
    "    \n",
    "    permutation_scores = [\n",
    "        _permutation_test_score(\n",
    "            estimator,\n",
    "            X,\n",
    "            _shuffle(y,i),\n",
    "            fit_params=fit_params,\n",
    "        )\n",
    "        for i in range(n_permutations)]\n",
    "    \n",
    "    permutation_scores = np.array(permutation_scores)\n",
    "    pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)\n",
    "    return score, permutation_scores, pvalue\n",
    "\n",
    "\n",
    "def _permutation_test_score(estimator, X, y, fit_params):\n",
    "    score = estimator.score(X,y)\n",
    "    return score\n",
    "\n",
    "\n",
    "def _shuffle(y, seed):\n",
    "    random.Random(seed).shuffle(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00ce1e5-4ec9-4ab1-b3d7-28a80d119ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting experiment and saving selected info do result df\n",
    "\n",
    "def run_experiment(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    preprocessed_pipeline,\n",
    "    X_test_df,\n",
    "    scale,\n",
    "    results_df,\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        # enter to grid search\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=3,\n",
    "        )\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        \n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "        mean_train_mae = grid_result.cv_results_[\"mean_train_neg_mean_absolute_error\"][best_estimator_index]\n",
    "        mean_train_mse = grid_result.cv_results_[\"mean_train_neg_mean_squared_error\"][best_estimator_index]\n",
    "       \n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           ± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train, cv=3\n",
    "        )\n",
    "        \n",
    "        pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "        estimator = grid_result.best_estimator_\n",
    "        score = estimator.score(pre_processed_test_X, y_test)\n",
    "        \n",
    "        # permutation test for external\n",
    "        pre_processed_test_X_copy = copy.deepcopy(pre_processed_test_X)\n",
    "        y_test_copy = copy.deepcopy(y_test)\n",
    "\n",
    "        score_ext, permutation_scores, pvalue_ext = permutation_test_score_external(estimator,\n",
    "            pre_processed_test_X_copy, y_test_copy, n_permutations=1000\n",
    "        )\n",
    "        \n",
    "        print(print(f\"     external validation r2: {score}      p-value:{pvalue_ext}\"))\n",
    "        \n",
    "\n",
    "        # insert selected info to df\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"mean_cv_mse\":mean_cv_neg_mean_squared_error,\n",
    "            \"std_cv_mse\": std_cv_neg_mean_squared_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"mean_train_mae\":mean_train_mae,\n",
    "            \"mean_train_mse\":mean_train_mse,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "            \"pre_processed_pipeline\": preprocessed_pipeline,\n",
    "            \"scale\": scale,\n",
    "            \"external_score\":score,\n",
    "            \"external_p-value\":pvalue_ext,\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8873e1df-d54b-4f3c-9d03-e56dde2fb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting experiment and saving selected info do result df. \n",
    "# Additionally estimate p-value of model's performance on full training set (no CV)\n",
    "\n",
    "def run_experiment_estimate_train_p(\n",
    "    tested_regressors,\n",
    "    regressor_params,\n",
    "    pipeline_name,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    dataset_name,\n",
    "    base_steps,\n",
    "    preprocessed_pipeline,\n",
    "    X_test_df,\n",
    "    scale,\n",
    "    results_df,\n",
    "):\n",
    "\n",
    "    for (regressor, params) in tested_regressors:\n",
    "        print(f\"Rating {regressor} \\n\")\n",
    "        tested_params = {**regressor_params, **params}\n",
    "\n",
    "        # enter to grid search\n",
    "        grid_result = rate_regressor(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            regressor,\n",
    "            tested_params,\n",
    "            base_steps,\n",
    "            cv=3,\n",
    "        )\n",
    "\n",
    "        best_estimator_index = grid_result.best_index_\n",
    "        mean_cv_r2 = grid_result.cv_results_[\"mean_test_r2\"][best_estimator_index]\n",
    "        std_cv_r2 = grid_result.cv_results_[\"std_test_r2\"][best_estimator_index]\n",
    "        mean_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_absolute_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_absolute_error\"\n",
    "        ][best_estimator_index]\n",
    "        mean_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"mean_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        std_cv_neg_mean_squared_error = grid_result.cv_results_[\n",
    "            \"std_test_neg_mean_squared_error\"\n",
    "        ][best_estimator_index]\n",
    "        \n",
    "        mean_train_r2 = grid_result.cv_results_[\"mean_train_r2\"][best_estimator_index]\n",
    "        mean_train_mae = grid_result.cv_results_[\"mean_train_neg_mean_absolute_error\"][best_estimator_index]\n",
    "        mean_train_mse = grid_result.cv_results_[\"mean_train_neg_mean_squared_error\"][best_estimator_index]\n",
    "       \n",
    "\n",
    "        print(f\"     Best parameters: {grid_result.best_params_}\")\n",
    "        print(f\"     mean r2: {mean_cv_r2}           ± {round(std_cv_r2,3)}\")\n",
    "        print(f\"     mean r2 train: {mean_train_r2}\")\n",
    "\n",
    "        cv_results = grid_result.cv_results_\n",
    "\n",
    "        # calculate p-value\n",
    "        scores_, pvalue_ = calculate_p_permutations(\n",
    "            grid_result.best_estimator_, X_train, y_train, cv=3\n",
    "        )\n",
    "        \n",
    "        pre_processed_test_X = preprocessed_pipeline.transform(X_test_df)\n",
    "        estimator = grid_result.best_estimator_\n",
    "        score = estimator.score(pre_processed_test_X, y_test)\n",
    "        \n",
    "        # permutation test for external\n",
    "        pre_processed_test_X_copy = copy.deepcopy(pre_processed_test_X)\n",
    "        y_test_copy = copy.deepcopy(y_test)\n",
    "\n",
    "        score_ext, permutation_scores, pvalue_ext = permutation_test_score_external(estimator,\n",
    "            pre_processed_test_X_copy, y_test_copy, n_permutations=1000\n",
    "        )\n",
    "        \n",
    "        # tests for whole train (to prove overfitting)\n",
    "        score_train, permutation_scores, pvalue_train = permutation_test_score_external(estimator,\n",
    "            X_train, y_train, n_permutations=1000\n",
    "        )\n",
    "        \n",
    "        print(print(f\"     external validation r2: {score}      p-value:{pvalue_ext}\"))\n",
    "        \n",
    "\n",
    "        # insert selected info to df\n",
    "        data = {\n",
    "            \"data_set\": dataset_name,\n",
    "            \"pipeline_name\": pipeline_name,\n",
    "            \"model\": regressor[0],\n",
    "            \"parameters\": grid_result.best_params_,\n",
    "            \"mean_cv_r2\": mean_cv_r2,\n",
    "            \"std_cv_r2\": std_cv_r2,\n",
    "            \"mean_cv_mae\": mean_cv_neg_mean_absolute_error,\n",
    "            \"std_cv_mae\": std_cv_neg_mean_absolute_error,\n",
    "            \"mean_cv_mse\":mean_cv_neg_mean_squared_error,\n",
    "            \"std_cv_mse\": std_cv_neg_mean_squared_error,\n",
    "            \"cv_results\": cv_results,\n",
    "            \"mean_train_r2\": mean_train_r2,\n",
    "            \"mean_train_mae\":mean_train_mae,\n",
    "            \"mean_train_mse\":mean_train_mse,\n",
    "            \"p-value\": pvalue_,\n",
    "            \"best_estimator\": grid_result.best_estimator_,\n",
    "            \"pre_processed_pipeline\": preprocessed_pipeline,\n",
    "            \"scale\": scale,\n",
    "            \"external_score\":score,\n",
    "            \"external_p-value\":pvalue_ext,\n",
    "            \"train_score\": score_train,\n",
    "            \"train_p-value\":pvalue_train,\n",
    "        }\n",
    "\n",
    "        results_df = results_df.append(data, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385045e-557e-44ab-a650-439c38ff8ab6",
   "metadata": {},
   "source": [
    "#### Global hyperparameters  of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f19073a0-ee3d-4ace-bbfd-177c753cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define estimators and their hyperparameters\n",
    "\n",
    "en = (\"en\", ElasticNet(random_state=random_state))\n",
    "en_params = dict(\n",
    "    en__alpha=np.logspace(-7, 3, num=20, base=10),\n",
    "    en__l1_ratio=np.logspace(-8, 0, num=17, base=10),\n",
    ")\n",
    "\n",
    "kr = (\"kr\", KernelRidge(kernel=\"rbf\"))\n",
    "kr_params = dict(\n",
    "    kr__alpha=np.logspace(-5, 3, num=20, base=10),\n",
    "    kr__gamma=np.logspace(-5, 3, num=20, base=10),\n",
    ")\n",
    "\n",
    "\n",
    "svr = (\"svr\", SVR())\n",
    "svr_params = dict(\n",
    "    svr__kernel=[\"linear\", \"rbf\"],\n",
    "    svr__C=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    svr__gamma=[\"scale\"],\n",
    "    svr__epsilon=[0.00001, 0.0001, 0.001, 0.01, 0.1, 1],\n",
    ")\n",
    "\n",
    "ln = (\"ln\", LinearRegression())\n",
    "ln_params = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fea081-c0ff-47c9-ba47-6323d0fa33bd",
   "metadata": {},
   "source": [
    "## Conduct experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931bd8b-511b-4111-a775-8db9ec102071",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 1: Classic ERP-wave analysis for ERN\n",
    "\n",
    "- error responses;\n",
    "- average over segments per subject;\n",
    "- time-window: 0 - 100 ms after response;\n",
    "- channels: FCz\n",
    "- feature: mean amplitude in selected time-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd23a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_regressors = [\n",
    "    (ln, ln_params),\n",
    "]\n",
    "\n",
    "regressor_params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14557c2f-30f6-4590-b88c-32c51ec32720",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_linear = pd.DataFrame()\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "this_bin=12\n",
    "\n",
    "for scale in scales:\n",
    "    print(f\"---------------------SCALE: {scale}-------------------------\")\n",
    "    \n",
    "    y_train = np.array(X_train_df[scale].to_list())\n",
    "    y_test = np.array(X_test_df[scale].to_list())\n",
    "\n",
    "    X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "\n",
    "    pipeline_name = f\"ERP_scale_{scale}\"\n",
    "\n",
    "    ############################################################################################\n",
    "    preprocessed_pipeline = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list=['FCz'])),\n",
    "        (\"trim\", EpochTrim(tmin=0, tmax=0.1)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "    ]).fit(X_train_df_copy)\n",
    "\n",
    "    preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "    features = Pipeline(steps = [\n",
    "        (\"mean_amplitude\", AverageSignal()),\n",
    "    ])\n",
    "\n",
    "    steps = [('features', features)]\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    regressor_steps = steps\n",
    "\n",
    "    # rate different models\n",
    "    results_ern_linear = run_experiment_train_p_val(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        preprocessed_X,\n",
    "        X_test_df_copy,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        regressor_steps,\n",
    "        preprocessed_pipeline,\n",
    "        X_test_df_copy,\n",
    "        scale,\n",
    "        results_ern_linear,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d212348-ef25-405c-b73d-fb34ea4ee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_linear.to_csv(\"../public_data/results/models/results/ERP_waves_analysis_train_cv_lin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db09a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Experiment 2: Classic ERP-wave analysis for Pe\n",
    "\n",
    "- error responses;\n",
    "- average over segments per subject;\n",
    "- time-window: 150 - 350 ms after response;\n",
    "- channels: CPz\n",
    "- feature: mean amplitude in selected time-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8743e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_regressors = [\n",
    "    (ln, ln_params),\n",
    "]\n",
    "\n",
    "regressor_params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03172c6-d0f3-4e64-9202-c85e362293e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pe_linear = pd.DataFrame()\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "this_bin=12\n",
    "\n",
    "for scale in scales:\n",
    "    print(f\"---------------------SCALE: {scale}-------------------------\")\n",
    "    \n",
    "    y_train = np.array(X_train_df[scale].to_list())\n",
    "    y_test = np.array(X_test_df[scale].to_list())\n",
    "\n",
    "    X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "    X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "\n",
    "    pipeline_name = f\"ERP_scale_{scale}\"\n",
    "\n",
    "    ############################################################################################\n",
    "    preprocessed_pipeline = Pipeline([\n",
    "        (\"channels_extraction\",PickChannels(channels_list=['CPz'])),\n",
    "        (\"trim\", EpochTrim(tmin=0.15, tmax=0.35)),\n",
    "        (\"average\", Evoked()),\n",
    "        ('extract_averaged_data', ExtractData()),\n",
    "    ]).fit(X_train_df_copy)\n",
    "\n",
    "    preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "\n",
    "    ###########################################################################################\n",
    "    features = Pipeline(steps = [\n",
    "        (\"mean_amplitude\", AverageSignal()),\n",
    "    ])\n",
    "\n",
    "    steps = [('features', features)]\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    regressor_steps = steps\n",
    "\n",
    "    # rate different models\n",
    "    results_pe_linear = run_experiment_train_p_val(\n",
    "        tested_regressors,\n",
    "        regressor_params,\n",
    "        pipeline_name,\n",
    "        preprocessed_X,\n",
    "        X_test_df_copy,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        dataset_name,\n",
    "        regressor_steps,\n",
    "        preprocessed_pipeline,\n",
    "        X_test_df_copy,\n",
    "        scale,\n",
    "        results_pe_linear,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "456b1d87-bdf7-4b83-9335-ccd316bac0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pe_linear.to_csv(\"../public_data/results/models/results/Pe_ERP_waves_analysis_train_cv_lin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a7e69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Experiment 3: PCA-based analysis for ERN\n",
    "\n",
    "- error responses;\n",
    "- average over segments per subject;\n",
    "- two ROIs;\n",
    "- spatial PCA feature extraction;\n",
    "- binning in 47ms-bins;\n",
    "- centralization of the signal to the ERP peak for each participant\n",
    "- time-window: two bins preceding the ERN peak to the one bin after the ERN peak; \n",
    "- feature: peak-to-peak amplitude in selected time-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "\n",
    "tested_regressors = [\n",
    "    (svr, svr_params), \n",
    "    (kr, kr_params), \n",
    "    (en, en_params)\n",
    "]\n",
    "\n",
    "regressor_params = dict()\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 5\n",
    "step_spatial_filter = 1\n",
    "\n",
    "\n",
    "roi_1 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"Cz\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "\n",
    "roi_2 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"F1\", \"Fz\", \"F2\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "roi_list = [roi_1, roi_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b6f2fe2-52ba-47f0-959a-9cc864f90de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wider signal, ERN: -2,1 Pe: 1,6\n",
    "import copy\n",
    "\n",
    "results_ern = pd.DataFrame()\n",
    "\n",
    "# manually test different numbers of spatial filter components\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "this_bin=12\n",
    "\n",
    "for scale in scales:\n",
    "    print(f\"---------------------SCALE: {scale}-------------------------\")\n",
    "    \n",
    "    y_train = np.array(X_train_df[scale].to_list())\n",
    "    y_test = np.array(X_test_df[scale].to_list())\n",
    "\n",
    "    for roi in roi_list:\n",
    "\n",
    "        X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "        X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "        print(f\"----------BOX: {roi}\")\n",
    "\n",
    "        for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter): \n",
    "            print(f\"---------------------SPATIAL FILTER : {n_components}-------------------------\")\n",
    "\n",
    "            pipeline_name = f\"PCA_{n_components}_scale_{scale}\"\n",
    "\n",
    "            ############################################################################################\n",
    "            preprocessed_pipeline = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_averaged_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "                (\"lowpass_filter\", LowpassFilter()),\n",
    "                (\"binning\", BinTransformer(step=this_bin)),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()) \n",
    "\n",
    "            ]).fit(X_train_df_copy)\n",
    "\n",
    "            preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "            ###########################################################################################\n",
    "\n",
    "            ern_features = Pipeline(steps=[\n",
    "                            (\"ern_data_extraction\", ErnTransformer()),\n",
    "                            (\"peak-to-peak\", ErnAmplitude2()),\n",
    "                            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                            (\"postprocessing\", PostprocessingTransformer()),\n",
    "                            (\"scaler\", StandardScaler()),\n",
    "            ])\n",
    "\n",
    "            ern_pe_features = FeatureUnion([\n",
    "                (\"ern_features\", ern_features), \n",
    "            ], n_jobs = 10)\n",
    "\n",
    "            steps = [('features', ern_pe_features)]\n",
    "\n",
    "            ############################################################################################\n",
    "\n",
    "            regressor_steps = steps\n",
    "\n",
    "            # rate different models\n",
    "            results_ern = run_experiment(\n",
    "                tested_regressors,\n",
    "                regressor_params,\n",
    "                pipeline_name,\n",
    "                preprocessed_X,\n",
    "                X_test_df_copy,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                dataset_name,\n",
    "                regressor_steps,\n",
    "                preprocessed_pipeline,\n",
    "                X_test_df_copy,\n",
    "                scale,\n",
    "                results_ern,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern.to_pickle(\"../public_data/results/models_pickles/regression_union_100-600_cached_ern_amplitude_various_scales_with_external_p.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b64653",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "### Experiment 4: PCA-based analysis for Pe\n",
    "\n",
    "- error responses;\n",
    "- average over segments per subject;\n",
    "- two ROIs;\n",
    "- spatial PCA feature extraction;\n",
    "- binning in 47ms-bins;\n",
    "- centralization of the signal to the ERP peak for each participant\n",
    "- time-window: from the first to the fifth bin after ERN peak; \n",
    "- feature: peak-to-peak amplitude in selected time-window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff013c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "\n",
    "tested_regressors = [\n",
    "    (svr, svr_params), \n",
    "    (kr, kr_params), \n",
    "    (en, en_params)\n",
    "]\n",
    "\n",
    "regressor_params = dict()\n",
    "\n",
    "min_spatial_filter = 3\n",
    "max_spatial_filter = 5\n",
    "step_spatial_filter = 1\n",
    "\n",
    "\n",
    "roi_1 = [\n",
    "    \"Fpz\", \n",
    "    \"AFz\",\n",
    "    \"Fz\",\n",
    "    \"FCz\",\n",
    "    \"C1\", \"Cz\",\"C2\",\n",
    "    \"CPz\",\n",
    "    \"P1\", \"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "roi_2 = [\n",
    "    \"Fpz\",\n",
    "    \"AFz\",\n",
    "    \"F1\",\"Fz\", \"F2\",\n",
    "    \"FC1\", \"FCz\", \"FC2\",\n",
    "    \"C1\",\"Cz\",\"C2\",\n",
    "    \"CP1\", \"CPz\", \"CP2\",\n",
    "    \"P1\",\"Pz\", \"P2\",\n",
    "]\n",
    "\n",
    "roi_list = [roi_1, roi_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd127b-08ab-4c64-b818-e96f797230c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pe = pd.DataFrame()\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" # Also affect subprocesses\n",
    "\n",
    "this_bin=12\n",
    "\n",
    "for scale in scales:\n",
    "    print(f\"---------------------SCALE: {scale}-------------------------\")\n",
    "    \n",
    "    y_train = np.array(X_train_df[scale].to_list())\n",
    "    y_test = np.array(X_test_df[scale].to_list())\n",
    "\n",
    "    for roi in roi_list:\n",
    "\n",
    "        X_train_df_copy = pd.DataFrame(copy.deepcopy(X_train_df.to_dict()))\n",
    "        X_test_df_copy = pd.DataFrame(copy.deepcopy(X_test_df.to_dict()))\n",
    "\n",
    "        print(f\"----------BOX: {roi}\")\n",
    "\n",
    "        for n_components in range(min_spatial_filter, max_spatial_filter, step_spatial_filter):\n",
    "            print(f\"---------------------SPATIAL FILTER : {n_components}-------------------------\")\n",
    "\n",
    "            pipeline_name = f\"PCA_{n_components}_scale_{scale}\"\n",
    "\n",
    "            ############################################################################################\n",
    "            preprocessed_pipeline = Pipeline([\n",
    "                (\"channels_extraction\",PickChannels(channels_list=roi)),\n",
    "                (\"average\", Evoked()),\n",
    "                ('extract_averaged_data', ExtractData()),\n",
    "                (\"spatial_filter_preprocessing\", SpatialFilterPreprocessing()),\n",
    "                (\"spatial_filter\",PCA(n_components=n_components, random_state=random_state)),\n",
    "                (\"spatial_filter_postprocessing\",SpatialFilterPostprocessing(timepoints_count=timepoints_count)),\n",
    "                (\"lowpass_filter\", LowpassFilter()),\n",
    "                (\"binning\", BinTransformer(step=this_bin)),\n",
    "                (\"baseline\", ErnBaselined()),\n",
    "                (\"centering\", CenteredSignalAfterBaseline3()) \n",
    "\n",
    "            ]).fit(X_train_df_copy)\n",
    "\n",
    "            preprocessed_X = preprocessed_pipeline.transform(X_train_df_copy)\n",
    "\n",
    "            ###########################################################################################\n",
    "\n",
    "            pe_features = Pipeline(steps = [\n",
    "                            (\"pe_data_extraction\", PeTransformer(3, 8)),\n",
    "                            (\"peak-to-peak\", PeAmplitude2()),\n",
    "                            (\"data_channel_swap\", ChannelDataSwap()),\n",
    "                            (\"postprocessing\", PostprocessingTransformer()),\n",
    "                            (\"scaler\", StandardScaler()),\n",
    "            ])\n",
    "\n",
    "            ern_pe_features = FeatureUnion([\n",
    "                (\"pe_features\", pe_features)\n",
    "            ], n_jobs = 10)\n",
    "\n",
    "            steps = [('features', ern_pe_features)]\n",
    "\n",
    "            ############################################################################################\n",
    "\n",
    "            regressor_steps = steps\n",
    "\n",
    "            # rate different models\n",
    "            results_pe = run_experiment(\n",
    "                tested_regressors,\n",
    "                regressor_params,\n",
    "                pipeline_name,\n",
    "                preprocessed_X,\n",
    "                X_test_df_copy,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                dataset_name,\n",
    "                regressor_steps,\n",
    "                preprocessed_pipeline,\n",
    "                X_test_df_copy,\n",
    "                scale,\n",
    "                results_pe,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68f0af-69b2-4dee-9cde-752cb97b41d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pe.to_pickle(\"../public_data/results/models_pickles/regression_union_100-600_cached_pe_amplitude_various_scales_with_external_p.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erpinator",
   "language": "python",
   "name": "erpinator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
